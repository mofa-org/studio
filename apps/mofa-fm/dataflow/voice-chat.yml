# Voice Chat Dataflow - Modular MoFA Integration
#
# Based on examples/conference/dataflow-study-audio-multi.yml
# Replaces debate-monitor, viewer, and audio-player with MoFA dynamic nodes
#
# Key features:
# - FIFO session queue in text segmenter
# - 3 different voices (Zhao Daniu, Chen Yifan, Luo Xiang)
# - Audio concatenation in arrival order
# - Complete backpressure control

nodes:
  # ============ Study Participants (MaaS) ============

  - id: student1
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-student1/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_student1.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  - id: student2
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-student2/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_student2.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  - id: tutor
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-tutor/text
      control: conference-controller/judge_prompt
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_tutor.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  # ============ Multi-Participant Audio Pipeline ============

  # Multi-Input Text Segmenter with FIFO Session Queue
  - id: multi-text-segmenter
    build: pip install -e ../../../node-hub/dora-text-segmenter
    path: dora-text-segmenter
    inputs:
      # Participant inputs - output names are derived from these input port names
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000

      # Audio complete signal from audio player (replaces TTS segment_complete)
      audio_complete:
        source: mofa-audio-player/audio_complete
        queue_size: 100

      # Audio buffer control for backpressure
      audio_buffer_control:
        source: mofa-audio-player/buffer_status
        queue_size: 10

      # Control signals (reset and cancel)
      control: conference-controller/llm_control
      reset: conference-controller/control_judge

    outputs:
      - text_segment_student1
      - text_segment_student2
      - text_segment_tutor
      - status
      - metrics
      - log

    env:
      SEGMENTER_MODE: "conference"
      SEGMENT_MODE: "sentence"
      MIN_SEGMENT_LENGTH: "5"
      MAX_SEGMENT_LENGTH: "15"
      PUNCTUATION_MARKS: '。！？.!?，,、；：""''（）【】《》'
      REMOVE_SPEAKER_ID: "true"
      LOG_LEVEL: "DEBUG"
      AUDIO_BUFFER_LOW_WATER_MARK: "30"
      AUDIO_BUFFER_HIGH_WATER_MARK: "60"

  # PrimeSpeech TTS for Student1 (Zhao Daniu - Male, Rational Voice)
  - id: primespeech-student1
    build: pip install -e ../../../node-hub/dora-common && pip install -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student1
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Zhao Daniu"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.0
      FRAGMENT_INTERVAL: "0.1"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # PrimeSpeech TTS for Student2 (Chen Yifan - Female, Emotional Voice)
  - id: primespeech-student2
    build: pip install -e ../../../node-hub/dora-common && pip install -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student2
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Chen Yifan"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.1
      FRAGMENT_INTERVAL: "0.2"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # PrimeSpeech TTS for Tutor (Luo Xiang - Male, Authoritative Voice)
  - id: primespeech-tutor
    build: pip install -e ../../../node-hub/dora-common && pip install -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_tutor
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Luo Xiang"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.1
      FRAGMENT_INTERVAL: "0.1"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # ============ 3 Conference Bridges (Switch) ============

  # Bridge 1: Student2 + Tutor → Student1
  - id: bridge-to-student1
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      control:
        source: conference-controller/control_llm1
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # Bridge 2: Student1 + Tutor → Student2
  - id: bridge-to-student2
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      control:
        source: conference-controller/control_llm2
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # Bridge 3: Student1 + Student2 → Tutor
  - id: bridge-to-tutor
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      control:
        source: conference-controller/control_judge
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # ============ Conference Controller (Brain) with Audio Backpressure ============

  - id: conference-controller
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-controller/Cargo.toml
    path: ../../../node-hub/dora-conference-controller/target/release/dora-conference-controller
    env:
      DORA_POLICY_PATTERN: "[(tutor, *), (student2, 1), (student1, 2)]"
      INITIAL_QUESTION_ID: 1
      AUDIO_BUFFER_THRESHOLD: 30
      AUDIO_BUFFER_RESUME_THRESHOLD: 10
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      control: mofa-prompt-input/control
      session_start: mofa-audio-player/session_start
      buffer_status: mofa-audio-player/buffer_status
    outputs:
      - control_judge
      - control_llm2
      - control_llm1
      - llm_control
      - judge_prompt
      - status
      - log

  # ============ MoFA Dynamic Nodes (UI Widgets) ============

  # Audio Player - receives synthesized audio and plays through speaker
  - id: mofa-audio-player
    path: dynamic
    inputs:
      audio_student1:
        source: primespeech-student1/audio
        queue_size: 1000
      audio_student2:
        source: primespeech-student2/audio
        queue_size: 1000
      audio_tutor:
        source: primespeech-tutor/audio
        queue_size: 1000
      control:
        source: conference-controller/llm_control
        queue_size: 10
    outputs:
      - buffer_status
      - status
      - session_start
      - audio_complete
      - log

  # Note: mofa-participant-panel has been consolidated into mofa-audio-player
  # AudioPlayerBridge now handles both audio playback AND LED level visualization

  # Prompt Input - receives LLM text outputs for chat display, sends user control
  - id: mofa-prompt-input
    path: dynamic
    env:
      DORA_STUDY_MODE: "true"
      LOG_LEVEL: "DEBUG"
    inputs:
      # LLM text outputs only (for chat display)
      llm1_text:
        source: student1/text
        queue_size: 1000
      llm2_text:
        source: student2/text
        queue_size: 1000
      judge_text:
        source: tutor/text
        queue_size: 1000
    outputs:
      - control

  # System Log - aggregates logs from all nodes
  - id: mofa-system-log
    path: dynamic
    inputs:
      # Student1 logs
      llm1_log:
        source: student1/log
        queue_size: 1000
      llm1_status: student1/status
      llm1_text:
        source: student1/text
        queue_size: 1000

      # Student2 logs
      llm2_log:
        source: student2/log
        queue_size: 1000
      llm2_status: student2/status
      llm2_text:
        source: student2/text
        queue_size: 1000

      # Tutor logs
      judge_log:
        source: tutor/log
        queue_size: 1000
      judge_status: tutor/status
      judge_text:
        source: tutor/text
        queue_size: 1000

      # Bridge logs
      bridge1_log:
        source: bridge-to-student1/log
        queue_size: 1000
      bridge1_status: bridge-to-student1/status
      bridge1_text:
        source: bridge-to-student1/text
        queue_size: 1000
      bridge2_log:
        source: bridge-to-student2/log
        queue_size: 1000
      bridge2_status: bridge-to-student2/status
      bridge2_text:
        source: bridge-to-student2/text
        queue_size: 1000
      bridge3_log:
        source: bridge-to-tutor/log
        queue_size: 1000
      bridge3_status: bridge-to-tutor/status
      bridge3_text:
        source: bridge-to-tutor/text
        queue_size: 1000

      # Controller logs
      controller_status: conference-controller/status
      controller_log:
        source: conference-controller/log
        queue_size: 1000
      control_judge: conference-controller/control_judge
      control_llm2: conference-controller/control_llm2
      control_llm1: conference-controller/control_llm1

      # Audio pipeline logs
      segmenter_log:
        source: multi-text-segmenter/log
        queue_size: 1000
      segmenter_status: multi-text-segmenter/status
      tts1_log:
        source: primespeech-student1/log
        queue_size: 1000
      tts1_status: primespeech-student1/status
      tts2_log:
        source: primespeech-student2/log
        queue_size: 1000
      tts2_status: primespeech-student2/status
      tts3_log:
        source: primespeech-tutor/log
        queue_size: 1000
      tts3_status: primespeech-tutor/status
      audio_status: mofa-audio-player/status
      audio_player_log:
        source: mofa-audio-player/log
        queue_size: 1000
